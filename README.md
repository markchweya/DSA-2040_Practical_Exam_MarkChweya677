
---

# DSA 2040 US 2025 End Semester Practical Exam â€“ Mark Chweya (ID: 677)

## ğŸ“Œ Overview

This repository contains my complete submission for the **DSA 2040 US 2025 End Semester Practical Exam**.
It covers **Data Warehousing** (Section 1) and **Data Mining** (Section 2) tasks as outlined in the exam paper.
All work is implemented in **Python** with supporting **SQL scripts**, **visualizations**, and **reports**.

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ DSA_2040_Practical_Exam.ipynb     # Main Jupyter Notebook with all code & markdown explanations
â”œâ”€â”€ retail_dw.db                      # SQLite database generated by ETL
â”œâ”€â”€ star_schema.png                   # Star schema diagram
â”œâ”€â”€ sales_by_country_quarter.png      # OLAP Roll-up visualization
â”œâ”€â”€ iris_pairplot.png                 # Iris dataset pairplot
â”œâ”€â”€ iris_heatmap.png                  # Iris feature correlation heatmap
â”œâ”€â”€ iris_boxplots.png                 # Iris feature boxplots
â”œâ”€â”€ kmeans_elbow_curve.png            # Elbow curve for K-Means
â”œâ”€â”€ kmeans_clusters.png               # Cluster visualization
â”œâ”€â”€ decision_tree_visualization.png   # Decision tree diagram
â”œâ”€â”€ association_rules.csv             # Extracted top 5 association rules
â”œâ”€â”€ README.md                         # This file
```

---

## ğŸ“Š Datasets Used

### **Section 1 â€“ Data Warehousing**

* **Synthetic Online Retail dataset** generated using Python and Faker library.
* 1,000 transactions across 5 countries, 100 unique customers, and 8 products in 3 categories.
* Stored in `retail_dw.db` as:

  * `CustomerDim`
  * `ProductDim`
  * `TimeDim`
  * `SalesFact`

### **Section 2 â€“ Data Mining**

* **Iris dataset** from scikit-learn.
* Preprocessed: handled missing values, scaled features with Min-Max, encoded target labels.

---

## â–¶ï¸ Run Instructions

1. Clone this repository:

   ```bash
   git clone https://github.com/<your-username>/DSA_2040_Practical_Exam_MarkChweya677.git
   cd DSA_2040_Practical_Exam_MarkChweya677
   ```
2. Install required Python libraries:

   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn faker mlxtend
   ```
3. Open the notebook:

   ```bash
   jupyter notebook DSA_2040_Practical_Exam.ipynb
   ```
4. Run all cells in order to:

   * Generate synthetic retail data
   * Perform ETL into SQLite
   * Run OLAP queries & visualizations
   * Preprocess and explore Iris dataset
   * Perform clustering, classification, and association rule mining

---

## ğŸ“ Self-Assessment

* **Section 1 â€“ Data Warehousing (50 marks)**

  * âœ… Star schema diagram created and explained.
  * âœ… ETL pipeline implemented with synthetic dataset.
  * âœ… OLAP queries executed with visualization and analysis report.
* **Section 2 â€“ Data Mining (50 marks)**

  * âœ… Data preprocessing & exploration completed.
  * âœ… K-Means clustering (k=2, 3, 4) with ARI, elbow curve, scatter plot.
  * âœ… Decision Tree and KNN classification with performance comparison.
  * âœ… Association rule mining with top 5 rules and retail implications.
* **Challenges:**

  * Had to generate synthetic datasets to match exam requirements.
  * Managed transformations to maintain data integrity for both sections.
* **Confidence:** The notebook runs end-to-end without errors and covers all exam tasks as described.

---


